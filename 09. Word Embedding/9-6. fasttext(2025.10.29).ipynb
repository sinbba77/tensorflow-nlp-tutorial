{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastText 실습.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK7zOfKku1uR"
      },
      "source": [
        "이 자료는 위키독스 딥 러닝을 이용한 자연어 처리 입문의 FastText 튜토리얼 자료입니다.  \n",
        "\n",
        "링크 : https://wikidocs.net/22883  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqCVq3IStj0b",
        "outputId": "2c469bf8-ea48-48ad-bcc5-66fab4d0575d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-uX7by_Nw5U"
      },
      "source": [
        "pip list | grep gensim"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw-A0mBCtoAB"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "from lxml import etree\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtkAw421toy4",
        "outputId": "b115c1de-ab4d-4a3a-b25d-c8577603a1ac"
      },
      "source": [
        "## 🌐 TED 강연 데이터(xml 파일)를 인터넷에서 다운로드하는 코드\n",
        "\n",
        "import urllib.request   # 인터넷에서 파일을 다운로드할 수 있는 모듈\n",
        "\n",
        "# urllib.request.urlretrieve(\"URL\", filename=\"저장할_파일이름\")\n",
        "# → 지정한 URL에 있는 데이터를 내 컴퓨터(Colab 환경)에 저장\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n",
        "    filename=\"ted_en-20160408.xml\"  # 다운로드 후 저장할 파일 이름\n",
        ")\n",
        "\n",
        "# ✅ 실행 결과:\n",
        "# 현재 작업 폴더에 \"ted_en-20160408.xml\" 파일이 생성됩니다.\n",
        "# (이 파일은 TED 영어 강연 자막 데이터셋으로, 텍스트 분류 실습에 자주 사용됩니다.)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x7fd874fac800>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBIBpnQDtpri"
      },
      "source": [
        "# 📘 TED 영어 강연 데이터(XML 파일)를 불러와서\n",
        "#     텍스트만 추출하고, 문장/단어 단위로 토큰화하는 전체 전처리 과정\n",
        "\n",
        "from lxml import etree        # XML 파일을 읽기 위한 라이브러리\n",
        "import re                     # 정규 표현식(특정 패턴 문자 찾기/제거)\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize  # 문장/단어 단위 토큰화 도구\n",
        "\n",
        "# ✅ 1️⃣ XML 파일 열기\n",
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "target_text = etree.parse(targetXML)   # XML 파일 구조로 파싱(읽어서 구조화)\n",
        "\n",
        "# ✅ 2️⃣ <content>...</content> 사이의 내용만 추출\n",
        "#   - TED 데이터에는 <content> 태그 안에 실제 강연 내용이 들어 있음\n",
        "#   - xpath('//content/text()') → 모든 <content> 태그의 텍스트를 리스트로 가져옴\n",
        "#   - '\\n'.join(...) → 여러 문장을 줄바꿈(\\n)으로 연결해 하나의 긴 문자열로 만듦\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# ✅ 3️⃣ (Audio), (Laughter) 등 괄호 안의 배경 설명 제거\n",
        "#   - re.sub(패턴, 대체문자, 문자열)\n",
        "#   - r'\\([^)]*\\)' → 괄호로 시작 '(' 후 ')' 전까지 모든 문자 제거\n",
        "#   - 즉, ( )로 둘러싸인 모든 내용 삭제\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# ✅ 4️⃣ 문장 단위로 나누기 (Sentence Tokenization)\n",
        "#   - sent_tokenize() : 문장을 마침표(.) 기준으로 분리\n",
        "#   - 예: \"Hello. I am a student.\" → [\"Hello.\", \"I am a student.\"]\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# ✅ 5️⃣ 각 문장별로 소문자 변환 + 구두점 제거\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "    # re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    # → 영어 알파벳(a~z)과 숫자(0~9)를 제외한 문자는 공백(\" \")으로 치환\n",
        "    # → 모든 대문자를 소문자로 바꿈 (lower())\n",
        "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "    normalized_text.append(tokens)\n",
        "# 결과 예: \"Hello, WORLD!!\" → \"hello world\"\n",
        "\n",
        "# ✅ 6️⃣ 단어 단위 토큰화 (Word Tokenization)\n",
        "#   - 각 문장을 다시 단어별로 분리\n",
        "#   - 예: [\"hello world\", \"this is nlp\"] → [[\"hello\", \"world\"], [\"this\", \"is\", \"nlp\"]]\n",
        "result = [word_tokenize(sentence) for sentence in normalized_text]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY3ZR568tswh",
        "outputId": "329c86d9-d6b8-4d47-c417-f2a62cb90836"
      },
      "source": [
        "print('총 샘플의 개수 : {}'.format(len(result)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 샘플의 개수 : 273424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV4cP8gAvd79",
        "outputId": "c5399091-9bbe-4091-a260-eaa8aea0845f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMD-u6oMNG0-"
      },
      "source": [
        "from gensim.models import Word2Vec, FastText"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVEZ38-TNOOd"
      },
      "source": [
        "# 🧠 Word2Vec 모델 학습 (TED 영어 강연 데이터 기반)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# ✅ Word2Vec 모델 생성 및 학습\n",
        "model = Word2Vec(\n",
        "    sentences=result,   # 학습에 사용할 문장(단어 리스트들의 리스트)\n",
        "    vector_size=100,    # 각 단어를 100차원 벡터로 표현 (단어 의미의 숫자 표현 크기)\n",
        "    window=5,           # 중심 단어를 기준으로 앞뒤 5개의 단어를 문맥으로 사용\n",
        "    min_count=5,        # 데이터 전체에서 5회 미만 등장한 단어는 무시\n",
        "    workers=4,          # 동시에 학습할 CPU 코어 수 (병렬 처리 → 학습 속도 향상)\n",
        "    sg=0                # 학습 방식: 0 = CBOW, 1 = Skip-gram\n",
        ")\n",
        "\n",
        "# ✅ 실행 결과:\n",
        "# - model 안에 각 단어의 의미 벡터가 학습됨\n",
        "# - model.wv['word'] 로 특정 단어의 벡터 확인 가능\n",
        "# - model.wv.most_similar('word') 로 비슷한 의미의 단어 확인 가능"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "C7XMiidTNTx4",
        "outputId": "9fdcc41e-0a71-4a03-aee1-ac17e56bd581"
      },
      "source": [
        "# 입력 단어에 대해서 유사한 단어를 찾아내는 코드에 electrofishing이라는 단어를 넣어봅니다.\n",
        "model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "# 에러 메시지는 단어 집합(Vocabulary)에 electrofishing이 존재하지 않는다고 합니다.\n",
        "# 이처럼 Word2Vec는 학습 데이터에 존재하지 않는 단어.\n",
        "# 즉, 모르는 단어에 대해서는 임베딩 벡터가 존재하지 않기 때문에 단어의 유사도를 계산할 수 없습니다."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'electrofishing' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2120353178.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 입력 단어에 대해서 유사한 단어를 찾아내는 코드에 electrofishing이라는 단어를 넣어봅니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 에러 메시지는 단어 집합(Vocabulary)에 electrofishing이 존재하지 않는다고 합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 이처럼 Word2Vec는 학습 데이터에 존재하지 않는 단어.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         all_keys = [\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'electrofishing' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2et3y-uNNuE"
      },
      "source": [
        "# ⚡ FastText 모델 학습 (TED 영어 강연 데이터 기반)\n",
        "# 실행 시간은 약 3분 정도 소요될 수 있습니다.\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "# ✅ FastText 모델 생성 및 학습\n",
        "model = FastText(\n",
        "    sentences=result,   # 학습용 데이터 (토큰화된 문장 리스트)\n",
        "    vector_size=100,    # 각 단어를 100차원 벡터로 표현\n",
        "    window=5,           # 중심 단어 기준으로 앞뒤 5단어를 문맥으로 사용\n",
        "    min_count=5,        # 5회 미만 등장하는 단어는 무시\n",
        "    workers=4,          # 학습에 사용할 CPU 코어 개수 (병렬 처리)\n",
        "    sg=1                # 학습 알고리즘 선택 (1=Skip-gram, 0=CBOW)\n",
        ")\n",
        "\n",
        "# ✅ FastText란?\n",
        "# - Word2Vec과 거의 비슷하지만, 단어를 “글자 단위(Subword)”까지 쪼개서 학습함\n",
        "# - 예: “apple” → [\"app\", \"ppl\", \"ple\"] 형태의 n-gram 조각으로 학습\n",
        "# - 따라서, 모르는 단어(OOV)도 구성 글자 정보를 이용해 벡터를 만들 수 있음\n",
        "\n",
        "# ✅ 실행 결과:\n",
        "# - model.wv['apple'] → \"apple\" 단어의 100차원 벡터\n",
        "# - model.wv.most_similar('apple') → \"apple\"과 비슷한 의미의 단어 출력"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Go_cquNU4F",
        "outputId": "5b275117-7ed5-4169-d061-0c6da6dc79cd"
      },
      "source": [
        "# Word2Vec은 학습하지 않은 단어에 대해서 유사한 단어를 찾아내지 못했지만,\n",
        "# FastText는 유사한 단어를 계산해서 출력하고 있음을 볼 수 있습니다.\n",
        "\n",
        "model.wv.most_similar(\"electrofishing\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('electrolyte', 0.8720206618309021),\n",
              " ('electrolux', 0.8692356944084167),\n",
              " ('electroshock', 0.8592448830604553),\n",
              " ('electro', 0.8582302927970886),\n",
              " ('electric', 0.8317796587944031),\n",
              " ('electron', 0.8255887031555176),\n",
              " ('electroencephalogram', 0.8247286081314087),\n",
              " ('electrochemical', 0.8222823143005371),\n",
              " ('electronic', 0.8212001323699951),\n",
              " ('electrogram', 0.8174077272415161)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}